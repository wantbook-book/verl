#!/usr/bin/env python3
# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
vLLM SAEé›†æˆåŠŸèƒ½æµ‹è¯•è„šæœ¬
éªŒè¯vLLM rolloutçš„SAEç‰¹å¾å åŠ æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import sys
from pathlib import Path
from tensordict import TensorDict
from omegaconf import DictConfig, OmegaConf

# æ·»åŠ VERLè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from verl import DataProto
from verl.workers.rollout.vllm_rollout.vllm_rollout_spmd import vLLMRollout


def create_mock_config(enable_sae=False):
    """åˆ›å»ºæ¨¡æ‹Ÿé…ç½®"""
    config = {
        "tensor_model_parallel_size": 1,
        "max_num_batched_tokens": 8192,
        "prompt_length": 100,
        "response_length": 10,
        "max_model_len": 200,
        "enable_chunked_prefill": False,
        "load_format": "dummy",
        "free_cache_engine": False,
        "dtype": "float16",
        "enforce_eager": True,
        "gpu_memory_utilization": 0.9,
        "disable_log_stats": True,
        "calculate_log_probs": False,
        "temperature": 1.0,
        "top_k": 50,
        "do_sample": True,
        "seed": 42,
    }
    
    if enable_sae:
        config["sae"] = {
            "enable": True,
            "model_path": "/fake/sae/path",  # æ¨¡æ‹Ÿè·¯å¾„
            "release": "test_release",
            "id": "test_id",
            "feature_idx": 1160,
            "strength_scale": 1.0,
            "max_activation": 5.0,
        }
    
    return DictConfig(config)


class MockTokenizer:
    """æ¨¡æ‹Ÿçš„tokenizer"""
    def __init__(self):
        self.pad_token_id = 0
        self.eos_token_id = 2


def test_vllm_sae_config_loading():
    """æµ‹è¯•vLLM SAEé…ç½®åŠ è½½"""
    print("ğŸ§ª æµ‹è¯•vLLM SAEé…ç½®åŠ è½½...")
    
    # æµ‹è¯•ä¸å¯ç”¨SAE
    config_no_sae = create_mock_config(enable_sae=False)
    tokenizer = MockTokenizer()
    
    try:
        rollout_no_sae = vLLMRollout(
            model_path="dummy_model",
            config=config_no_sae,
            tokenizer=tokenizer,
            model_hf_config=None
        )
        
        assert rollout_no_sae.sae is None
        assert not rollout_no_sae.sae_config
        print("âœ… ä¸å¯ç”¨SAEçš„é…ç½®åŠ è½½æ­£å¸¸")
        
    except Exception as e:
        print(f"âš ï¸ ä¸å¯ç”¨SAEçš„é…ç½®åŠ è½½å¤±è´¥: {e}")
    
    # æµ‹è¯•å¯ç”¨SAEï¼ˆä½†æ²¡æœ‰çœŸå®çš„SAEæ¨¡å‹ï¼‰
    config_with_sae = create_mock_config(enable_sae=True)
    
    try:
        rollout_with_sae = vLLMRollout(
            model_path="dummy_model",
            config=config_with_sae,
            tokenizer=tokenizer,
            model_hf_config=None
        )
        
        # ç”±äºæ²¡æœ‰çœŸå®çš„SAEæ¨¡å‹ï¼Œåº”è¯¥åŠ è½½å¤±è´¥ä½†ä¸ä¼šå´©æºƒ
        print(f"SAEåŠ è½½çŠ¶æ€: {rollout_with_sae.sae is not None}")
        print(f"SAEé…ç½®: {rollout_with_sae.sae_config}")
        print("âœ… å¯ç”¨SAEçš„é…ç½®åŠ è½½å®Œæˆï¼ˆå¯èƒ½æ²¡æœ‰çœŸå®SAEæ¨¡å‹ï¼‰")
        
    except Exception as e:
        print(f"âš ï¸ å¯ç”¨SAEçš„é…ç½®åŠ è½½å¤±è´¥: {e}")


def test_sae_meta_info_processing():
    """æµ‹è¯•SAE meta_infoå¤„ç†"""
    print("\nğŸ§ª æµ‹è¯•SAE meta_infoå¤„ç†...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    seq_len = 20
    
    input_ids = torch.randint(1, 1000, (batch_size, seq_len))
    attention_mask = torch.ones_like(input_ids)
    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)
    
    batch = TensorDict({
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "position_ids": position_ids,
    }, batch_size=batch_size)
    
    # æµ‹è¯•ä¸åŒçš„SAE meta_infoé…ç½®
    test_configs = [
        {
            "name": "æ— SAE",
            "meta_info": {
                "eos_token_id": [2],
                "pad_token_id": 0,
                "sae_enabled": False,
            }
        },
        {
            "name": "å¯ç”¨SAE - å•ç‰¹å¾å¹²é¢„",
            "meta_info": {
                "eos_token_id": [2],
                "pad_token_id": 0,
                "sae_enabled": True,
                "sae_feature_idx": 1160,
                "sae_strength_scale": 1.0,
                "sae_max_activation": 5.0,
                "sae_steering_layer": -1,
            }
        },
        {
            "name": "å¯ç”¨SAE - å…¨å±€å¹²é¢„",
            "meta_info": {
                "eos_token_id": [2],
                "pad_token_id": 0,
                "sae_enabled": True,
                "sae_strength_scale": 0.5,
                "sae_steering_layer": 10,
            }
        },
    ]
    
    for test_config in test_configs:
        print(f"\n   æµ‹è¯•é…ç½®: {test_config['name']}")
        
        prompts = DataProto(batch=batch, meta_info=test_config["meta_info"])
        
        # æ¨¡æ‹ŸSAEå‚æ•°æå–é€»è¾‘
        sae_enabled = prompts.meta_info.get("sae_enabled", False)
        
        if sae_enabled:
            sae_params = {
                'sae_feature_idx': prompts.meta_info.get('sae_feature_idx'),
                'sae_strength_scale': prompts.meta_info.get('sae_strength_scale', 1.0),
                'sae_max_activation': prompts.meta_info.get('sae_max_activation'),
                'sae_steering_layer': prompts.meta_info.get('sae_steering_layer', -1),
            }
            print(f"   âœ… SAEå‚æ•°æå–æˆåŠŸ: {sae_params}")
        else:
            print("   âœ… æ­£ç¡®è¯†åˆ«ä¸ºéSAEæ¨¡å¼")


def test_sae_hook_functions():
    """æµ‹è¯•SAE hookå‡½æ•°"""
    print("\nğŸ§ª æµ‹è¯•SAE hookå‡½æ•°...")
    
    # æµ‹è¯•hookå‡½æ•°çš„å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½
    try:
        from verl.workers.rollout.vllm_rollout.vllm_rollout_spmd import (
            get_intervention_hook, 
            get_clamp_hook, 
            add_hooks
        )
        print("âœ… SAE hookå‡½æ•°å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•add_hooksä¸Šä¸‹æ–‡ç®¡ç†å™¨
        class MockModule:
            def register_forward_hook(self, hook_fn):
                class MockHandle:
                    def remove(self):
                        pass
                return MockHandle()
        
        mock_module = MockModule()
        mock_hook_fn = lambda module, input, output: output
        
        hook_context = add_hooks([], [(mock_module, mock_hook_fn)])
        
        with hook_context:
            print("âœ… Hookä¸Šä¸‹æ–‡ç®¡ç†å™¨å·¥ä½œæ­£å¸¸")
        
        print("âœ… Hookç§»é™¤æ­£å¸¸")
        
    except ImportError as e:
        print(f"âš ï¸ SAE hookå‡½æ•°å¯¼å…¥å¤±è´¥: {e}")
    except Exception as e:
        print(f"âš ï¸ SAE hookå‡½æ•°æµ‹è¯•å¤±è´¥: {e}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹vLLM SAEé›†æˆæµ‹è¯•")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥SAEä¾èµ–
        try:
            from verl.workers.rollout.vllm_rollout.vllm_rollout_spmd import SAE_AVAILABLE
            if SAE_AVAILABLE:
                print("âœ… SAE-Lenså¯ç”¨")
            else:
                print("âš ï¸ SAE-Lensä¸å¯ç”¨ï¼Œå°†è·³è¿‡éƒ¨åˆ†æµ‹è¯•")
        except ImportError:
            print("âš ï¸ æ— æ³•æ£€æŸ¥SAEä¾èµ–")
        
        # è¿è¡Œæµ‹è¯•
        test_vllm_sae_config_loading()
        test_sae_meta_info_processing()
        test_sae_hook_functions()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ vLLM SAEé›†æˆæµ‹è¯•å®Œæˆï¼")
        print("\nğŸ’¡ æ³¨æ„äº‹é¡¹:")
        print("- å®é™…ä½¿ç”¨éœ€è¦çœŸå®çš„SAEæ¨¡å‹å’ŒvLLMç¯å¢ƒ")
        print("- å½“å‰æµ‹è¯•ä¸»è¦éªŒè¯é…ç½®åŠ è½½å’Œå‚æ•°å¤„ç†é€»è¾‘")
        print("- å®Œæ•´åŠŸèƒ½æµ‹è¯•éœ€è¦åœ¨å®é™…çš„vLLMéƒ¨ç½²ç¯å¢ƒä¸­è¿›è¡Œ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
