#!/usr/bin/env python3
# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
SAE HooksåŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•æ”¹è¿›åçš„SAE hookså®ç°ï¼Œå‚è€ƒSAE-Reasoning2
"""

import torch
import sys
from pathlib import Path

# æ·»åŠ VERLè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from verl.workers.rollout.vllm_rollout.vllm_rollout_spmd import (
        GlobalSAE,
        get_intervention_hook,
        get_clamp_hook, 
        get_multi_intervention_hook,
        add_hooks
    )
    HOOKS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Hookå‡½æ•°å¯¼å…¥å¤±è´¥: {e}")
    HOOKS_AVAILABLE = False


class MockSAE:
    """æ¨¡æ‹Ÿçš„SAEæ¨¡å‹"""
    def __init__(self, hidden_size=1024, num_features=512):
        self.hidden_size = hidden_size
        self.num_features = num_features
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ¨¡æ‹ŸSAEæƒé‡
        self.W_enc = torch.randn(hidden_size, num_features, device=self.device) * 0.1
        self.W_dec = torch.randn(num_features, hidden_size, device=self.device) * 0.1
        self.b_enc = torch.zeros(num_features, device=self.device)
        self.b_dec = torch.zeros(hidden_size, device=self.device)
        
        class MockConfig:
            hook_layer = 10
        self.cfg = MockConfig()
    
    def to(self, device):
        self.device = device
        self.W_enc = self.W_enc.to(device)
        self.W_dec = self.W_dec.to(device)
        self.b_enc = self.b_enc.to(device)
        self.b_dec = self.b_dec.to(device)
        return self
    
    def encode(self, x):
        """ç¼–ç ï¼šhidden states -> features"""
        # x: [batch, seq_len, hidden_size] -> [batch, seq_len, num_features]
        return torch.relu((x @ self.W_enc) + self.b_enc)
    
    def decode(self, features):
        """è§£ç ï¼šfeatures -> hidden states"""
        # features: [batch, seq_len, num_features] -> [batch, seq_len, hidden_size]
        return (features @ self.W_dec) + self.b_dec


class MockModule(torch.nn.Module):
    """æ¨¡æ‹Ÿçš„ç¥ç»ç½‘ç»œæ¨¡å—"""
    def __init__(self, hidden_size=1024):
        super().__init__()
        self.hidden_size = hidden_size
        self.linear = torch.nn.Linear(hidden_size, hidden_size)
    
    def forward(self, x):
        return self.linear(x)


def test_global_sae_control():
    """æµ‹è¯•GlobalSAEæ§åˆ¶æœºåˆ¶"""
    print("ğŸ§ª æµ‹è¯•GlobalSAEæ§åˆ¶æœºåˆ¶...")
    
    # æµ‹è¯•é»˜è®¤çŠ¶æ€
    assert GlobalSAE.use_sae == True, "GlobalSAEé»˜è®¤åº”è¯¥å¯ç”¨"
    print("âœ… GlobalSAEé»˜è®¤å¯ç”¨")
    
    # æµ‹è¯•ç¦ç”¨
    GlobalSAE.use_sae = False
    assert GlobalSAE.use_sae == False, "GlobalSAEç¦ç”¨å¤±è´¥"
    print("âœ… GlobalSAEç¦ç”¨æˆåŠŸ")
    
    # é‡æ–°å¯ç”¨
    GlobalSAE.use_sae = True
    assert GlobalSAE.use_sae == True, "GlobalSAEé‡æ–°å¯ç”¨å¤±è´¥"
    print("âœ… GlobalSAEé‡æ–°å¯ç”¨æˆåŠŸ")


def test_intervention_hook():
    """æµ‹è¯•å¹²é¢„hook"""
    print("\nğŸ§ª æµ‹è¯•å¹²é¢„hook...")
    
    if not HOOKS_AVAILABLE:
        print("âš ï¸ Hookå‡½æ•°ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, seq_len, hidden_size = 2, 10, 1024
    num_features = 512
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    sae = MockSAE(hidden_size, num_features).to(device)
    module = MockModule(hidden_size).to(device)
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    input_data = torch.randn(batch_size, seq_len, hidden_size, device=device)
    
    # æµ‹è¯•å•ç‰¹å¾å¹²é¢„
    feature_idx = 100
    strength = 2.0
    max_activation = 5.0
    
    hook_fn = get_intervention_hook(sae, feature_idx=feature_idx, strength=strength, max_activation=max_activation)
    
    # åº”ç”¨hook
    with add_hooks([], [(module, hook_fn)]):
        output = module(input_data)
        
        print(f"âœ… å•ç‰¹å¾å¹²é¢„hookåº”ç”¨æˆåŠŸ")
        print(f"   è¾“å…¥å½¢çŠ¶: {input_data.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   ç‰¹å¾ç´¢å¼•: {feature_idx}, å¼ºåº¦: {strength}")
    
    # æµ‹è¯•å…¨å±€å¹²é¢„
    global_strength = 0.8
    global_hook_fn = get_intervention_hook(sae, strength=global_strength)
    
    with add_hooks([], [(module, global_hook_fn)]):
        output = module(input_data)
        print(f"âœ… å…¨å±€å¹²é¢„hookåº”ç”¨æˆåŠŸï¼Œå¼ºåº¦: {global_strength}")


def test_clamp_hook():
    """æµ‹è¯•é’³åˆ¶hook"""
    print("\nğŸ§ª æµ‹è¯•é’³åˆ¶hook...")
    
    if not HOOKS_AVAILABLE:
        print("âš ï¸ Hookå‡½æ•°ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, seq_len, hidden_size = 2, 10, 1024
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    module = MockModule(hidden_size).to(device)
    
    # åˆ›å»ºæ–¹å‘å‘é‡
    direction = torch.randn(hidden_size, device=device)
    max_activation = 3.0
    strength = 1.5
    
    hook_fn = get_clamp_hook(direction, max_activation, strength)
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    input_data = torch.randn(batch_size, seq_len, hidden_size, device=device)
    
    # åº”ç”¨hook
    with add_hooks([], [(module, hook_fn)]):
        output = module(input_data)
        
        print(f"âœ… é’³åˆ¶hookåº”ç”¨æˆåŠŸ")
        print(f"   æ–¹å‘å‘é‡å½¢çŠ¶: {direction.shape}")
        print(f"   æœ€å¤§æ¿€æ´»å€¼: {max_activation}, å¼ºåº¦: {strength}")


def test_multi_intervention_hook():
    """æµ‹è¯•å¤šç‰¹å¾å¹²é¢„hook"""
    print("\nğŸ§ª æµ‹è¯•å¤šç‰¹å¾å¹²é¢„hook...")
    
    if not HOOKS_AVAILABLE:
        print("âš ï¸ Hookå‡½æ•°ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, seq_len, hidden_size = 2, 10, 1024
    num_features = 512
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    sae = MockSAE(hidden_size, num_features).to(device)
    module = MockModule(hidden_size).to(device)
    
    # å¤šç‰¹å¾å‚æ•°
    feature_idxs = [50, 100, 200]
    max_activations = [3.0, 4.0, 2.5]
    strengths = [1.0, 1.5, 0.8]
    
    hook_fn = get_multi_intervention_hook(sae, feature_idxs, max_activations, strengths)
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    input_data = torch.randn(batch_size, seq_len, hidden_size, device=device)
    
    # åº”ç”¨hook
    with add_hooks([], [(module, hook_fn)]):
        output = module(input_data)
        
        print(f"âœ… å¤šç‰¹å¾å¹²é¢„hookåº”ç”¨æˆåŠŸ")
        print(f"   ç‰¹å¾ç´¢å¼•: {feature_idxs}")
        print(f"   æœ€å¤§æ¿€æ´»å€¼: {max_activations}")
        print(f"   å¼ºåº¦: {strengths}")


def test_hook_disable():
    """æµ‹è¯•hookç¦ç”¨åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•hookç¦ç”¨åŠŸèƒ½...")
    
    if not HOOKS_AVAILABLE:
        print("âš ï¸ Hookå‡½æ•°ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, seq_len, hidden_size = 2, 10, 1024
    num_features = 512
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    sae = MockSAE(hidden_size, num_features).to(device)
    module = MockModule(hidden_size).to(device)
    
    input_data = torch.randn(batch_size, seq_len, hidden_size, device=device)
    
    # åˆ›å»ºhook
    hook_fn = get_intervention_hook(sae, feature_idx=100, strength=2.0)
    
    # å¯ç”¨çŠ¶æ€ä¸‹çš„è¾“å‡º
    GlobalSAE.use_sae = True
    with add_hooks([], [(module, hook_fn)]):
        output_enabled = module(input_data)
    
    # ç¦ç”¨çŠ¶æ€ä¸‹çš„è¾“å‡º
    GlobalSAE.use_sae = False
    with add_hooks([], [(module, hook_fn)]):
        output_disabled = module(input_data)
    
    # æ¢å¤å¯ç”¨çŠ¶æ€
    GlobalSAE.use_sae = True
    
    print("âœ… Hookç¦ç”¨åŠŸèƒ½æµ‹è¯•å®Œæˆ")
    print(f"   å¯ç”¨æ—¶è¾“å‡ºå‡å€¼: {output_enabled.mean():.4f}")
    print(f"   ç¦ç”¨æ—¶è¾“å‡ºå‡å€¼: {output_disabled.mean():.4f}")


def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\nğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†...")
    
    if not HOOKS_AVAILABLE:
        print("âš ï¸ Hookå‡½æ•°ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    # æµ‹è¯•æ— æ•ˆç‰¹å¾ç´¢å¼•
    try:
        sae = MockSAE(1024, 512)
        hook_fn = get_intervention_hook(sae, feature_idx=1000)  # è¶…å‡ºèŒƒå›´
        print("âœ… æ— æ•ˆç‰¹å¾ç´¢å¼•å¤„ç†æ­£å¸¸ï¼ˆä¸ä¼šç«‹å³æŠ¥é”™ï¼‰")
    except Exception as e:
        print(f"âš ï¸ æ— æ•ˆç‰¹å¾ç´¢å¼•å¤„ç†å¼‚å¸¸: {e}")
    
    # æµ‹è¯•è®¾å¤‡ä¸åŒ¹é…
    try:
        sae = MockSAE(1024, 512).to('cpu')
        module = MockModule(1024)
        input_data = torch.randn(2, 10, 1024)
        
        if torch.cuda.is_available():
            input_data = input_data.cuda()
            module = module.cuda()
        
        hook_fn = get_intervention_hook(sae, feature_idx=100)
        
        with add_hooks([], [(module, hook_fn)]):
            output = module(input_data)
        
        print("âœ… è®¾å¤‡ä¸åŒ¹é…è‡ªåŠ¨å¤„ç†æ­£å¸¸")
    except Exception as e:
        print(f"âš ï¸ è®¾å¤‡ä¸åŒ¹é…å¤„ç†å¼‚å¸¸: {e}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹SAE HooksåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        test_global_sae_control()
        test_intervention_hook()
        test_clamp_hook()
        test_multi_intervention_hook()
        test_hook_disable()
        test_error_handling()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ SAE HooksåŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
        print("\nğŸ’¡ æµ‹è¯•æ€»ç»“:")
        print("- âœ… GlobalSAEæ§åˆ¶æœºåˆ¶æ­£å¸¸")
        print("- âœ… å•ç‰¹å¾å¹²é¢„hookæ­£å¸¸")
        print("- âœ… é’³åˆ¶hookæ­£å¸¸")
        print("- âœ… å¤šç‰¹å¾å¹²é¢„hookæ­£å¸¸")
        print("- âœ… Hookç¦ç”¨åŠŸèƒ½æ­£å¸¸")
        print("- âœ… é”™è¯¯å¤„ç†æœºåˆ¶æ­£å¸¸")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
