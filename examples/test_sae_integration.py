#!/usr/bin/env python3
# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
SAEé›†æˆåŠŸèƒ½æµ‹è¯•è„šæœ¬
éªŒè¯SAEç‰¹å¾å åŠ æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import sys
from pathlib import Path
from tensordict import TensorDict

# æ·»åŠ VERLè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from verl import DataProto
from verl.workers.rollout.naive.naive_rollout import NaiveRollout


class MockModel(torch.nn.Module):
    """æ¨¡æ‹Ÿçš„LLMæ¨¡å‹ç”¨äºæµ‹è¯•"""
    
    def __init__(self, vocab_size=32000, hidden_size=4096):
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.embed_tokens = torch.nn.Embedding(vocab_size, hidden_size)
        self.layers = torch.nn.ModuleList([
            torch.nn.TransformerDecoderLayer(
                d_model=hidden_size, 
                nhead=32,
                batch_first=True
            ) for _ in range(2)
        ])
        self.lm_head = torch.nn.Linear(hidden_size, vocab_size)
    
    def forward(self, input_ids, attention_mask=None, position_ids=None, output_hidden_states=False):
        # ç®€å•çš„å‰å‘ä¼ æ’­
        x = self.embed_tokens(input_ids)
        
        hidden_states = []
        for layer in self.layers:
            if output_hidden_states:
                hidden_states.append(x)
            # ç®€åŒ–çš„transformerå±‚
            x = layer(x, x)
        
        if output_hidden_states:
            hidden_states.append(x)
        
        logits = self.lm_head(x)
        
        # è¿”å›ç±»ä¼¼HuggingFaceçš„è¾“å‡º
        class Output:
            def __init__(self, logits, hidden_states=None):
                self.logits = logits
                self.hidden_states = hidden_states if hidden_states else None
        
        return Output(logits, hidden_states if output_hidden_states else None)


class MockConfig:
    """æ¨¡æ‹Ÿçš„é…ç½®ç±»"""
    def __init__(self):
        self.response_length = 10
        self.temperature = 1.0
        self.top_k = 50
        self.do_sample = True


def test_sae_integration():
    """æµ‹è¯•SAEé›†æˆåŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•SAEé›†æˆåŠŸèƒ½...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œé…ç½®
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = MockModel().to(device)
    config = MockConfig()
    
    # åˆ›å»ºrollout
    rollout = NaiveRollout(model, config)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    seq_len = 20
    vocab_size = 32000
    
    input_ids = torch.randint(1, 1000, (batch_size, seq_len)).to(device)
    attention_mask = torch.ones_like(input_ids).to(device)
    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1).to(device)
    
    batch = TensorDict({
        "input_ids": input_ids,
        "attention_mask": attention_mask, 
        "position_ids": position_ids,
    }, batch_size=batch_size)
    
    # æµ‹è¯•1: ä¸å¯ç”¨SAE
    print("\nğŸ“‹ æµ‹è¯•1: æ ‡å‡†ç”Ÿæˆï¼ˆä¸å¯ç”¨SAEï¼‰")
    meta_info_standard = {
        "eos_token_id": [2],
        "pad_token_id": 0,
        "sae_enabled": False,
    }
    
    prompts_standard = DataProto(batch=batch, meta_info=meta_info_standard)
    
    try:
        output_standard = rollout.generate_sequences(prompts_standard)
        print("âœ… æ ‡å‡†ç”ŸæˆæˆåŠŸ")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output_standard.batch['responses'].shape}")
        print(f"   SAEå¯ç”¨: {output_standard.meta_info.get('sae_enabled', False)}")
    except Exception as e:
        print(f"âŒ æ ‡å‡†ç”Ÿæˆå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•2: å¯ç”¨SAE
    print("\nğŸ“‹ æµ‹è¯•2: SAEå¢å¼ºç”Ÿæˆ")
    meta_info_sae = {
        "eos_token_id": [2],
        "pad_token_id": 0,
        "sae_enabled": True,
        "sae_num_features": 512,
        "sae_strength_scale": 1.0,
        "sae_steering_layer": -1,
    }
    
    prompts_sae = DataProto(batch=batch, meta_info=meta_info_sae)
    
    try:
        output_sae = rollout.generate_sequences(prompts_sae)
        print("âœ… SAEå¢å¼ºç”ŸæˆæˆåŠŸ")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output_sae.batch['responses'].shape}")
        print(f"   SAEå¯ç”¨: {output_sae.meta_info.get('sae_enabled', False)}")
        
        if "sae_strengths" in output_sae.meta_info:
            strengths = output_sae.meta_info["sae_strengths"]
            print(f"   å¼ºåº¦ç»Ÿè®¡: mean={strengths.mean():.4f}, std={strengths.std():.4f}")
            print(f"   å¼ºåº¦å½¢çŠ¶: {strengths.shape}")
        
    except Exception as e:
        print(f"âŒ SAEå¢å¼ºç”Ÿæˆå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•3: æ¯”è¾ƒè¾“å‡ºå·®å¼‚
    print("\nğŸ“‹ æµ‹è¯•3: è¾“å‡ºå·®å¼‚åˆ†æ")
    
    try:
        # æ¯”è¾ƒç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦ä¸åŒï¼ˆåº”è¯¥æœ‰å·®å¼‚ï¼Œå› ä¸ºSAEæ”¹å˜äº†ç”Ÿæˆè¿‡ç¨‹ï¼‰
        responses_standard = output_standard.batch["responses"]
        responses_sae = output_sae.batch["responses"]
        
        # è®¡ç®—å·®å¼‚
        differences = (responses_standard != responses_sae).float().mean()
        print(f"   Tokenå·®å¼‚ç‡: {differences:.2%}")
        
        if differences > 0:
            print("âœ… SAEç¡®å®å½±å“äº†ç”Ÿæˆç»“æœ")
        else:
            print("âš ï¸ SAEä¼¼ä¹æ²¡æœ‰å½±å“ç”Ÿæˆç»“æœï¼ˆå¯èƒ½æ˜¯éšæœºæ€§å¯¼è‡´ï¼‰")
            
    except Exception as e:
        print(f"âš ï¸ è¾“å‡ºæ¯”è¾ƒå¤±è´¥: {e}")
    
    print("\nğŸ‰ SAEé›†æˆæµ‹è¯•å®Œæˆï¼")
    return True


def test_sae_config_propagation():
    """æµ‹è¯•SAEé…ç½®ä¼ æ’­"""
    print("\nğŸ§ª æµ‹è¯•SAEé…ç½®ä¼ æ’­...")
    
    # æµ‹è¯•ä¸åŒçš„SAEé…ç½®
    test_configs = [
        {"sae_enabled": True, "sae_strength_scale": 0.5},
        {"sae_enabled": True, "sae_strength_scale": 2.0},
        {"sae_enabled": False},
    ]
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = MockModel().to(device)
    config = MockConfig()
    rollout = NaiveRollout(model, config)
    
    # åˆ›å»ºåŸºç¡€æ•°æ®
    batch_size = 1
    seq_len = 10
    
    input_ids = torch.randint(1, 1000, (batch_size, seq_len)).to(device)
    attention_mask = torch.ones_like(input_ids).to(device)
    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1).to(device)
    
    batch = TensorDict({
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "position_ids": position_ids,
    }, batch_size=batch_size)
    
    for i, test_config in enumerate(test_configs):
        print(f"\n   é…ç½® {i+1}: {test_config}")
        
        meta_info = {
            "eos_token_id": [2],
            "pad_token_id": 0,
            **test_config
        }
        
        prompts = DataProto(batch=batch, meta_info=meta_info)
        
        try:
            output = rollout.generate_sequences(prompts)
            sae_enabled = output.meta_info.get("sae_enabled", False)
            print(f"   âœ… ç”ŸæˆæˆåŠŸï¼ŒSAEçŠ¶æ€: {sae_enabled}")
            
            if sae_enabled and "sae_strengths" in output.meta_info:
                strengths = output.meta_info["sae_strengths"]
                print(f"   ğŸ“Š å¼ºåº¦èŒƒå›´: [{strengths.min():.2f}, {strengths.max():.2f}]")
                
        except Exception as e:
            print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
    
    print("âœ… é…ç½®ä¼ æ’­æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹SAEé›†æˆæµ‹è¯•")
    print("="*50)
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import torch
        import tensordict
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    except ImportError as e:
        print(f"âŒ ä¾èµ–ç¼ºå¤±: {e}")
        sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    success = True
    
    try:
        success &= test_sae_integration()
        test_sae_config_propagation()
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        success = False
    
    print("\n" + "="*50)
    if success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼SAEé›†æˆåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        sys.exit(1)
